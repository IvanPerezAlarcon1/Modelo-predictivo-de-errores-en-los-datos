----------------------COSAS POR HACER-------------------
- Se podría particionar la tabla UNIQUE_VALUES_STRING_COLUMNS para optimizar tiempos de carga, o agregar indices
- INDICADOR DE SI ACEPTA O NO NUEVOS VALORES LA COLUMNA, mediante el ingreso por archivo de ingreso

- Ver el escenario donde se ingresen las mismas columnas, pero en distinto orden

- Si existe alguna columna que tenga más de un 10% de valores nulos, el conjunto completo no debería ingresarse a la tabla final, ya que no sería representativo hacer algún tipo de acción con
información tan vaga

+ programar el que pueda tener una tabla de bdd como entrada
+ programar escenarios de depuracion de outliers tukey y dixon
+ Programar un indicador, que se corte el programa completo si es que el dataframe posee más de un 10% de nulos en una columna luego de dejar el aviso del caso, por ahora solo se corta el for que recorre
las columnas pero el programa sigue, lo cual no debería pasar.


+ Hacer las validaciones de indicadores de si la columna acepta outliers(col numericas) y/o nulos(string/numericas), si los acepta, deja pasar las columnas
 NOTA: Se podría validar este caso con la columna que tiene casi todos los valores nulos, podría simplemente aceptarla así
+ Hacer la retroalimentación del diccionario de datos cuando se ingresen datos al diccionario
+ ind = 0 #si una funcion de imputación detecta un break debido a que una columna string o numerica posee un % de nulos > 10%, se termina el programa 
+ Falta ir actualizando los indicadores con la data histórica depurada.


+ Ver como hacer que al eliminar una columna del diccionario de datos, si es string, se borren los valores únicos para esa columna, o hacer
que no la tome, en caso de no requerirla más. Eliminacion en cascada de la tabla UNIQUE_VALUES
+ Ver si agregar la expresión regular como validación a las columnas string
+ Ver si se agrega el IRQ al diccionario
+ El analista podría ingresar valores únicos a la tabla en caso de requerirse, actualmente se haría mediante un insert
+ Se podría consultar el tipo  de archivo de entrada, si es excel, si es .csv(y su sparador), o si la entrada será una tabla de bdd(ingresar el nombre y esquema)


ERRORES: CUANDO VOY A CORREGIR OUTLIERS, EN ADR IMPUTÉ UN OUTLIER Y EL PROGRAMA ARROJÓ ERROR, PORQUE LA CURTOSIS EN LA BDD ERA MAYOR A MIL, PERO EN EL DATASET INGRESADO ERA DE 1.8, 
ENTONCES AL INTENTAR CORREGIR LOS OUTLIERS SE DETECTABAN MUCHOS Y ARROJÓ ERROR.


-----------------YA REALIZADOS--------------------------- (LAS COSAS TERMINADAS SE LISTAN EN ESTE APARTADO DE LO TERMINADO RECIENTEMENTE ARRIBA Y LO TERMINADO HACE MAS TIEMPO, ESTÁ MÁS ABAJO)
+ VER COMO HACER EL INGRESO DE DATOS LUEGO DEL PRIMER INGRESO Y CONTEXTUALIZACION, YA QUE SE DEBEN COMPROBAR LOS CAMPOS DEL DF DE ENTRADA Y LOS DE LA TABLA DESTINO
+ #FALTA CORROBORAR QUE EL DATAFRAME INGRESADO ESTÉ CONTEXTUALIZADO EFECTIVAMENTE, SI NO LO ESTÁ, DEBERÍA CREARSE, SI ESTÁ CONTEXTUALIZADO, SE DEBERÍA EJECUTAR EL CÓDIGO DE AQUÍ 
  Y DEJAR EL DF RESULTANTE EN LA TABLA FINAL EN LA BDD
+ Para el primer ingreso de datos, no se puede especificar si una columna puede o no tener más del 10% de nulos, en tal caso se podría:
	- Contextualizar el modelo pero no ingresar la tabla histórica
	- Contextualizar el modelo e ingresar los datos de igual forma con la columna que tiene un alto % de nulos.
	- Se podrían dejar pasar por defecto los nulos en la contextualización, luego por defecto no dejarlos más, a menos que en el diccionario se especifique
	  manualmente lo contrario.
	+ SE DEJO EN EL PRIMER INGRESO PASAR LA COLUMNA SI ES QUE TIENE MAS DE UN 10% DE NULOS, PERO A PARTIR DEL 2DO INGRESO NO SE PERMITE
+ Acorde a lo anterior, corregir el break, en la funcion input_df_numerico(df,df_num) del archivo -> imputaciones_functions.py
+ Validar escenario donde se ingrese un conjunto con nombres de columnas distintos a los registrados o las mismas columnas más otras o algunas pero no todas de las registradas
+ Hacer el main de correcciones una vez constextualizado el prototipo
+ Hacer el almacenamiento de la tabla histórica de datos depurados del contexto.
- Para n grams, podría ocupar un algoritmo para cadenas de más de una palabra y otro para cadenas de una sola palabra, o simplemente
abarcar campos de más de 1 palabra. Programar este segundo escenario primero para avanzar
- Probar los tipos de duplicados con las funciones que se quieren usar

- Hacer funcion que me traiga los valores únicos de la columna string que se está analizando
- Ver como hacer las funciones de imputación de datos para avanzar, tomar en cuenta que se revisa por columna
- Ver comparación de columnas del conjunto de datos de entrada con las del diccionario para detectar cuales están registradas y cuales no
- Hacer funcion que saque los ngrams de una cadena y otra que calcule el indice de similitud
+ EN EL PRIMER INGRESO DE DATOS, TAMBIEN DEBERÍA REGISTRAR LOS INDICADORES, LUEGO CORREGIR POR PRIMERA VEZ LOS DATOS Y GUARDARLOS EN EL HISTÓRICO -> (YA HECHO)
--ya se arreglaron las columnas string, Funciona con columnas que tengan problemas de letras extra letras faltantes, funciona mal
cuando se tienen problemas de orden de palabras en una cadena