----------------------COSAS POR HACER-------------------
- Se podría particionar la tabla UNIQUE_VALUES_STRING_COLUMNS para optimizar tiempos de carga, o agregar indices
- INDICADOR DE SI ACEPTA O NO NUEVOS VALORES LA COLUMNA, mediante el ingreso por archivo de ingreso

- Ver el escenario donde se ingresen las mismas columnas, pero en distinto orden

+ programar escenarios de depuracion de outliers tukey y dixon

+ Ver como hacer que al eliminar una columna del diccionario de datos, si es string, se borren los valores únicos para esa columna, o hacer
que no la tome, en caso de no requerirla más. Eliminacion en cascada de la tabla UNIQUE_VALUES
+ Ver si agregar la expresión regular como validación a las columnas string
+ El analista podría ingresar valores únicos a la tabla en caso de requerirse, actualmente se haría mediante un insert


ERRORES: CUANDO VOY A CORREGIR OUTLIERS, EN ADR IMPUTÉ UN OUTLIER Y EL PROGRAMA ARROJÓ ERROR, PORQUE LA CURTOSIS EN LA BDD ERA MAYOR A MIL, PERO EN EL DATASET INGRESADO ERA DE 1.8, 
ENTONCES AL INTENTAR CORREGIR LOS OUTLIERS SE DETECTABAN MUCHOS Y ARROJÓ ERROR.

--------------------------------PROBLEMAS QUE FALTAN POR RESOLVER---------------------------------------
- Para outliers, en caso de que se utilice tukey, de todas formas me equivoqué al mencionar que tukey era la óptima para el elcenario en que IRQ = 0, según la investigación esa es Grubbs,
	- Se podría corregir el informe a que si IRQ = 0 se usa grubbs y en caso contrario Tukey, luego en las pruebas probar con tukey, mencionar que no tiene muy buen desempeño, probar 
	  con Grubbs para cuando IRQ != 0 y dejarlo como tecnica detectora de outliers para ambos escenarios predefinida.

--------------------------------------------------------------------------------------------------------



-----------------YA REALIZADOS--------------------------- (LAS COSAS TERMINADAS SE LISTAN EN ESTE APARTADO DE LO TERMINADO RECIENTEMENTE ARRIBA Y LO TERMINADO HACE MAS TIEMPO, ESTÁ MÁS ABAJO)
- Si existe alguna columna que tenga más de un 10% de valores nulos, el conjunto completo no debería ingresarse a la tabla final, ya que no sería representativo hacer algún tipo de acción con
información tan vaga -> por esta versión se deja ingresar al conjunto final pero se da aviso de la existencia de más del 10% de valores nulos
+ Hacer las validaciones de indicadores de si la columna acepta outliers(col numericas) y/o nulos(string/numericas), si los acepta, deja pasar las columnas
+ Se podría consultar el tipo  de archivo de entrada, si es excel, si es .csv(y su sparador), o si la entrada será una tabla de bdd(ingresar el nombre y esquema) -> se hizo un menú
+ programar el que pueda tener una tabla de bdd como entrada
+ Ver si se agrega el IRQ al diccionario -> NO se agrega
+ Falta ir actualizando los indicadores con la data histórica depurada.
+ Hacer la retroalimentación del diccionario de datos cuando se ingresen datos al diccionario (se activa una vez se ingresen los nuevos datos a la tabla histórica)
+ VER COMO HACER EL INGRESO DE DATOS LUEGO DEL PRIMER INGRESO Y CONTEXTUALIZACION, YA QUE SE DEBEN COMPROBAR LOS CAMPOS DEL DF DE ENTRADA Y LOS DE LA TABLA DESTINO
+ #FALTA CORROBORAR QUE EL DATAFRAME INGRESADO ESTÉ CONTEXTUALIZADO EFECTIVAMENTE, SI NO LO ESTÁ, DEBERÍA CREARSE, SI ESTÁ CONTEXTUALIZADO, SE DEBERÍA EJECUTAR EL CÓDIGO DE AQUÍ 
  Y DEJAR EL DF RESULTANTE EN LA TABLA FINAL EN LA BDD
+ Para el primer ingreso de datos, no se puede especificar si una columna puede o no tener más del 10% de nulos, en tal caso se podría:
	- Contextualizar el modelo pero no ingresar la tabla histórica
	- Contextualizar el modelo e ingresar los datos de igual forma con la columna que tiene un alto % de nulos.
	- Se podrían dejar pasar por defecto los nulos en la contextualización, luego por defecto no dejarlos más, a menos que en el diccionario se especifique
	  manualmente lo contrario.
	+ SE DEJO EN EL PRIMER INGRESO PASAR LA COLUMNA SI ES QUE TIENE MAS DE UN 10% DE NULOS, PERO A PARTIR DEL 2DO INGRESO NO SE PERMITE
+ Acorde a lo anterior, corregir el break, en la funcion input_df_numerico(df,df_num) del archivo -> imputaciones_functions.py
+ Validar escenario donde se ingrese un conjunto con nombres de columnas distintos a los registrados o las mismas columnas más otras o algunas pero no todas de las registradas
+ Hacer el main de correcciones una vez constextualizado el prototipo
+ Hacer el almacenamiento de la tabla histórica de datos depurados del contexto.
- Para n grams, podría ocupar un algoritmo para cadenas de más de una palabra y otro para cadenas de una sola palabra, o simplemente
abarcar campos de más de 1 palabra. Programar este segundo escenario primero para avanzar
- Probar los tipos de duplicados con las funciones que se quieren usar

- Hacer funcion que me traiga los valores únicos de la columna string que se está analizando
- Ver como hacer las funciones de imputación de datos para avanzar, tomar en cuenta que se revisa por columna
- Ver comparación de columnas del conjunto de datos de entrada con las del diccionario para detectar cuales están registradas y cuales no
- Hacer funcion que saque los ngrams de una cadena y otra que calcule el indice de similitud
+ EN EL PRIMER INGRESO DE DATOS, TAMBIEN DEBERÍA REGISTRAR LOS INDICADORES, LUEGO CORREGIR POR PRIMERA VEZ LOS DATOS Y GUARDARLOS EN EL HISTÓRICO -> (YA HECHO)
--ya se arreglaron las columnas string, Funciona con columnas que tengan problemas de letras extra letras faltantes, funciona mal
cuando se tienen problemas de orden de palabras en una cadena